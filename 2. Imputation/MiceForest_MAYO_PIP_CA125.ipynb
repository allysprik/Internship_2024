{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Author: Ally Sprik\n",
    "### Last-updated: 25-02-2024\n",
    "\n",
    "Goal of this notebook is to use MICEForest to impute the missing values in the MAYO dataset. The imputation will be based on the PIPENDO dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b669ccb17939b90b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import miceforest as mf\n",
    "import random\n",
    "import sklearn.neighbors._base \n",
    "import sys\n",
    "\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from sklearn.impute import KNNImputer\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.copy_on_write = True  # This will allow the code to run faster and keep Pandas happy. Technical detail: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_MAYO = pd.read_csv(\"../../0. Source_files/0.2. Cleaned_data/MAYO_cleaned_model.csv\")\n",
    "df_PIP = pd.read_csv('../../0. Source_files/0.2. Cleaned_data/Casper_PIPENDO_Cleaned.csv')\n",
    "\n",
    "# columns not in PIPENDO dataset fill in with NA\n",
    "for col in df_MAYO.columns:\n",
    "    if col not in df_PIP.columns:\n",
    "        df_PIP[col] = np.nan\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f9f7c14e65e2da8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the columns to be used for the imputation. The columns are evidence columns for the Bayesian network. The columns are:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfa0402045e4671f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evidence_columns = [\"ER\", \"PR\", \"p53\", \"L1CAM\", \"CA125\", \"Platelets\", \"PreoperativeGrade\", \"LVSI\", \"MyometrialInvasion\"]\n",
    "\n",
    "# Replace the 0 and 1 with no and yes\n",
    "df_MAYO = df_MAYO[evidence_columns].replace({0:'no', 1:'yes'})\n",
    "df_PIP = df_PIP[evidence_columns].replace({0:'no', 1:'yes'})\n",
    "\n",
    "# Concatenate the two datasets first MAYO then PIPENDO\n",
    "data = pd.concat([df_MAYO, df_PIP], axis=0, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1731d0017f190e6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the data types of the columns to category. This is needed for the MICEForest imputation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbb77925adbaee43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for column in evidence_columns:\n",
    "    df_MAYO[column] = df_MAYO[column].astype('category')\n",
    "    df_PIP[column] = df_PIP[column].astype('category')\n",
    "    data[column] = data[column].astype('category')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44570aef9b180be8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the imputation model, using the PIP dataset as the training data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14a865e60b10c013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Set-up the imputation model\n",
    "kds = mf.ImputationKernel(\n",
    "    data=df_PIP,\n",
    "    random_state=42,\n",
    "    categorical_feature='auto',\n",
    "    \n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2cdb4d88232f2f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the imputation model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c79898d975199a61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kds.mice(50, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4595b2b7006c0754"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Impute the missing values in the MAYO dataset, with the PIPENDO trained model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2014a66bf9e3a15b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "completed_data = kds.impute_new_data(df_MAYO)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b2936b71e6e4d05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Retrieve the data and check if the original and imputed data are the same where the value was not missing.\n",
    "\n",
    "Pseudocode:\n",
    "- For each column in the evidence columns\n",
    "    - temp is the original MAYO data, with all the missing values removed\n",
    "    - index is the index of the temp\n",
    "    - temppart is the imputed data, with the same index as the temp\n",
    "    \n",
    "    - Compare if temp and temppart are the same\n",
    "        - If they are the same, print that the column is the same\n",
    "        - If they are not the same, print that the column is not the same"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f361266ef4f1ca05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAYO_part = completed_data.complete_data()\n",
    "\n",
    "for col in evidence_columns:\n",
    "    temp = df_MAYO[col].dropna()\n",
    "    index = temp.index\n",
    "    temppart = MAYO_part.loc[index, col]\n",
    "    \n",
    "    # Compare if its the same\n",
    "    if (temp == temppart).all():\n",
    "        print(f\"{col} is the same\")\n",
    "    else:\n",
    "        print(f\"{col} is not the same\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ba293c8cca838c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in original MAYO dataset and add the imputed values\n",
    "\n",
    "Pseudocode:\n",
    "- Load in the original MAYO dataset\n",
    "- Add the imputed CA125 values to the original MAYO dataset\n",
    "- For each column in the imputed dataset\n",
    "    - If the column is not in the original MAYO dataset\n",
    "        - Add the column to the original MAYO dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "714380d950a90f0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAYO_w_CA125 = pd.read_csv(\"../../0. Source_files/0.2. Cleaned_data/MAYO_cleaned_model.csv\")\n",
    "MAYO_w_CA125['CA125'] = MAYO_part['CA125']\n",
    "\n",
    "for col in MAYO_w_CA125.columns:\n",
    "    if col not in MAYO_part.columns:\n",
    "        MAYO_part[col] = MAYO_w_CA125[col]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2fdba53ab8778a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the imputed datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1827da093367f15c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAYO_w_CA125.to_csv('../../0. Source_files/0.3. Imputed_data/MayoCA125_wPIP_MiceForest.csv', index=False)\n",
    "MAYO_part.to_csv('../../0. Source_files/0.3. Imputed_data/Mayo_wPIP_fullimp_MiceForest.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "558127d84f858b6e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
