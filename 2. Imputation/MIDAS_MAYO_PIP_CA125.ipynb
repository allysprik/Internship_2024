{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Author: Ally Sprik\n",
    "### Last-updated: 25-02-2024\n",
    "\n",
    "Goal of this notebook is to utilise MIDASpy to impute the missing values in the MAYO dataset, using PIPENDO. The imputed dataset will be used in the next steps of the analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad6894f22ce9fa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import MIDASpy as midas\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# get gpu available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "pd.options.mode.copy_on_write = True  # This will allow the code to run faster and keep Pandas happy. Technical detail: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_MAYO = pd.read_csv(\"../../0. Source_files/0.2. Cleaned_data/MAYO_cleaned_model.csv\")\n",
    "df_PIP = pd.read_csv('../../0. Source_files/0.2. Cleaned_data/Casper_PIPENDO_Cleaned.csv', index_col='Unnamed: 0')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f9f7c14e65e2da8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select the columns to use as imputation evidence. The columns are the same for both datasets, so we can use the same columns for both datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2279c00f2e194056"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evidence_columns = [\"ER\", \"PR\", \"p53\", \"L1CAM\", \"CA125\", \"Platelets\", \"PreoperativeGrade\",\"LNM\", \"LVSI\", \"Chemotherapy\", \"Radiotherapy\", \"Survival1yr\", \"Survival3yr\", \"Survival5yr\", \"Cytology\"]\n",
    "\n",
    "\n",
    "df_MAYO = df_MAYO[evidence_columns]\n",
    "df_PIP = df_PIP[evidence_columns]\n",
    "\n",
    "# Concatenate the two datasets first MAYO then PIPENDO\n",
    "data = pd.concat([df_MAYO, df_PIP], axis=0, ignore_index=True).replace({0:'no', 1:'yes'})\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1731d0017f190e6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the data to categorical"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f2a551f02125ddf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    data[column] = data[column].astype('category')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44570aef9b180be8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encode the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e29f6328aef46771"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded, cat_cols_list = midas.cat_conv(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fd1cd7e830f4b7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the MIDAS model and train it\n",
    "\n",
    "Parameters:\n",
    "- layer_structure: list of integers, the number of nodes in each layer of the autoencoder\n",
    "- vae_layer: boolean, whether to include a variational autoencoder layer\n",
    "- seed: int, random seed\n",
    "- input_drop: float, the dropout rate for the input layer\n",
    "- training_epochs: int, the number of epochs to train the model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93008eaf977ce806"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imputer = midas.Midas(layer_structure=[256,256,256], vae_layer=True, seed=123, input_drop=0.90, latent_space_size=64, vae_sample_var=0.8, vae_alpha=1)\n",
    "\n",
    "imputer.build_model(encoded, softmax_columns=cat_cols_list)\n",
    "imputer.train_model(training_epochs=100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4595b2b7006c0754"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate the imputations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f46b96aeb83553c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imputations = imputer.generate_samples(m=10).output_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b2936b71e6e4d05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decode the imputations\n",
    "\n",
    "Pseudocode:\n",
    "- Flatten the list of imputations\n",
    "- Create a list of the categorical columns\n",
    "- for each imputation:\n",
    "    - create a dataframe with the imputed values\n",
    "    - remove the original categorical columns\n",
    "    - add the new categorical columns\n",
    "- for each imputation:\n",
    "    - for each column in the imputation:\n",
    "        - for each row in the column:\n",
    "            - remove the prefix of the column name\n",
    "- Retrieve the last imputation as the completed data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f73eeb0c08f874d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flat_cats = [cat for variable in cat_cols_list for cat in variable]\n",
    "categorical = data.columns.values\n",
    "\n",
    "for i in range(len(imputations)):\n",
    "    tmp_cat = [imputations[i][x].idxmax(axis=1) for x in cat_cols_list]\n",
    "    cat_df = pd.DataFrame({categorical[i]: tmp_cat[i] for i in range(len(categorical))})\n",
    "    imputations[i] = pd.concat([imputations[i], cat_df], axis=1).drop(flat_cats, axis=1)\n",
    "for i in range(0, 10):\n",
    "    imputation = imputations[i]\n",
    "    for col in imputation.columns.values:\n",
    "        for j in range(len(imputation)):\n",
    "            imputations.loc[i, col].loc[j] = imputation[col][j].removeprefix(col + '_')\n",
    "            \n",
    "completed_data = imputations[9]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f87c1d722b332b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the imputed data back into the original datasets, check if the imputed data is the same as the original data where there were values\n",
    "\n",
    "Pseudocode:\n",
    "- Define the mayo part as the rows up until the length of the original MAYO dataset\n",
    "- Define the PIP part as the rows from the length of the original MAYO dataset to the end of the imputed data\n",
    "- for each column in the evidence columns:\n",
    "    - create a temporary dataframe with the column from the original MAYO dataset, missing values removed\n",
    "    - get the index of the non-missing values\n",
    "    - get the corresponding values from the MAYO part of the imputed data\n",
    "    - compare if the two dataframes are the same"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4098be80e5ee47e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAYO_part = completed_data.iloc[:len(df_MAYO), :]\n",
    "PIP_part = completed_data.iloc[len(df_MAYO):, :]\n",
    "\n",
    "for col in evidence_columns:\n",
    "    temp = df_MAYO[col].dropna()\n",
    "    index = temp.index\n",
    "    temppart = MAYO_part[col].iloc[index]\n",
    "    \n",
    "    # Compare if its the same\n",
    "    if (temp == temppart).all():\n",
    "        print(f\"{col} is the same\")\n",
    "    else:\n",
    "        print(f\"{col} is not the same\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7485107191e2190"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the original data and add the imputed ca125 to it\n",
    "\n",
    "Pseudocode:\n",
    "- Load the original MAYO data\n",
    "- Add the imputed CA125 to the original data\n",
    "- For each column in the imputed data:\n",
    "    - if the column is not in the original data:\n",
    "        - add the column to the original data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9d3be493b3b484f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAYO_w_CA125 = pd.read_csv(\"../0. Source_files/0.2. Cleaned_data/MAYO_subdag.csv\")\n",
    "MAYO_w_CA125['CA125'] = MAYO_part['CA125']\n",
    "\n",
    "for col in MAYO_w_CA125.columns:\n",
    "    if col not in MAYO_part.columns:\n",
    "        MAYO_part[col] = MAYO_w_CA125[col]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2fdba53ab8778a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the imputed data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63850e3a8983af5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAYO_w_CA125.to_csv('../../0. Source_files/0.3. Imputed_data/MayoCA125_wPIP_MidasPy.csv', index=False)\n",
    "MAYO_part.to_csv('../../0. Source_files/0.3. Imputed_data/Mayo_wPIP_fullimp_MidasPy.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "558127d84f858b6e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
